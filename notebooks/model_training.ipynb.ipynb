{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spSn0u_jPzcZ"
   },
   "source": [
    "<div dir=\"rtl\">\n",
    "<font size=5>\n",
    "Improving Image Quality (Super-Resolution) with Custom CNN\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "<li>‚úÖ <strong>Data Preparation:</strong> Use a standard dataset (CIFAR-10) and create data pairs (poor image, high quality image) by downsampling the original images.</li>\n",
    "<li>‚úÖ <strong>Building a Super-Resolution Model:</strong> Design a custom CNN architecture that includes layers to increase the image dimensions such as <strong><code>Conv2DTranspose</code></strong> or <strong><code>UpSampling2D</code></strong>.</li>\n",
    "<li>‚úÖ <strong>Choosing an appropriate Loss Function:</strong> Use functions such as <code>Mean Squared Error (MSE)</code> or <code>Mean Absolute Error (MAE)</code> To compare the generated image pixel by pixel with the original image.</li>\n",
    "<li>‚úÖ <strong>Model training and validation:</strong> Train the model to minimize the difference between the reconstructed image and the real high-quality image.</li>\n",
    "<li>‚úÖ <strong>Result evaluation and visualization:</strong> Display a triplet of images (low-quality input, model output, and high-quality target) to visually evaluate the model's performance.</li>\n",
    "</font>\n",
    "</font>\n",
    "</div>\n",
    "<br>\n",
    "<div dir=\"rtl\">\n",
    "<h2><strong>Dataset</strong></h2>\n",
    "<li> We use the <strong>CIFAR-10</strong> dataset, which is available by default in Keras and does not require manual download.</li>\n",
    "<li>This dataset contains <strong>60,000 color images</strong> with dimensions <code>32x32</code> pixels. We will use these images as high-resolution versions and produce the low-resolution versions ourselves.</li>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6v4TxWG5JsoN"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">1. Load the required libraries</font> </div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gMruIlLVPvmh"
   },
   "outputs": [],
   "source": [
    "# TODO: Import the required libraries\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghXfbT1dQH_O"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">2. Loading and Preparing the Dataset</font> </div>\n",
    "<br>\n",
    "<div dir=\"rtl\">\n",
    "<font size=4>\n",
    "In this section, we load the CIFAR-10 dataset and prepare it for the Super-Resolution problem.\n",
    "<br><br>\n",
    "<strong>Steps:</strong>\n",
    "<li>1. Load the CIFAR-10 dataset. We only need the images (x_train, x_test) and the labels (y_train, y_test) are not relevant in this exercise.</li>\n",
    "<li>2. Normalize the pixel values ‚Äã‚Äãto the range <code>[0, 1]</code>. This will help in the stability of the model training.</li>\n",
    "<li>3. Load the Cifar10 dataset into the project via Tensorflow.</li>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kqhdW175QGaS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the CIFAR-10 dataset.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data: (50000, 32, 32, 3)\n",
      "Shape of test data: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the original data\n",
    "print('Shape of training data:', x_train.shape)\n",
    "print('Shape of test data:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqFHN0t4Wl21"
   },
   "source": [
    "<br>\n",
    "<div dir=\"rtl\">\n",
    "<font size=4>\n",
    "Convert the pixel values ‚Äã‚Äãto float32 and normalize them to the range [0, 1].\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cDjyhQpuQGX2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel value after normalization - minimum: 0.0 Maximum: 1.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Convert pixel values ‚Äã‚Äãto float32 and normalize to the range [0, 1].\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "print('Pixel value after normalization - minimum:', x_train.min(), 'Maximum:', x_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA2I7RFXWuls"
   },
   "source": [
    "<br />\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "In this section, the dimensions of the high-resolution (<strong>High Resolution - HR</strong\n",
    ">) and low-resolution (<strong>Low Resolution - LR</strong>) images are specified.\n",
    "The original images are <strong>32√ó32</strong>, but to simulate the low-resolution version, we reduce the same images to <strong>16√ó16</strong> dimensions.\n",
    "This helps the model to learn\n",
    "how to reconstruct clearer and more accurate versions from smaller, less detailed images.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "d-G9NcBjKsrO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-Resolution image dimensions (model target): (50000, 32, 32, 3)\n",
      "Low-Resolution image dimensions (model input): (50000, 16, 16, 3)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define dimensions of high-quality (HR) and low-quality (LR) images\n",
    "\n",
    "\n",
    "def create_hr_lr_pairs(images, lr_size=(16, 16)):\n",
    "    \"\"\"\n",
    "    Create High-Resolution and Low-Resolution data pairs\n",
    "\n",
    "    Parameters:\n",
    "    - images: Original images with dimensions 32x32\n",
    "    - lr_size: Dimensions of Low-Resolution images (default: 16x16)\n",
    "    - hr_size: Dimensions of High-Resolution images (default: 32x32)\n",
    "    \"\"\"\n",
    "    # Create Low-Resolution images by reducing dimensions\n",
    "    lr_images = tf.image.resize(images, lr_size, method='bicubic')\n",
    "\n",
    "\n",
    "\n",
    "    return lr_images, images # (model input, training target)\n",
    "\n",
    "# Create training data pairs\n",
    "x_train_lr, x_train_hr = create_hr_lr_pairs(x_train)\n",
    "x_test_lr, x_test_hr = create_hr_lr_pairs(x_test)\n",
    "\n",
    "print('High-Resolution image dimensions (model target):', x_train.shape)\n",
    "print('Low-Resolution image dimensions (model input):', x_train_lr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AanGa2u8QM6M"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">3. ‚Äã‚ÄãGenerating pairs of low-quality and high-quality data</font> </div>\n",
    "<br>\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "Our model needs to learn how to create a high-quality image (High Resolution (HR)) from a low-quality image (Low Resolution (LR)). To do this, we need to prepare the training data in the form of pairs (LR, HR).\n",
    "<br><br>\n",
    "<strong>Task:</strong>\n",
    "<li>Using the <code>tf.image</code> function, reduce the high-quality images to half their size (e.g. from 32x32 to 16x16) to create low-quality (LR) versions of them.</li>\n",
    "<li>These LR images will be used as input to the model and the HR images will be used as target output.</li>\n",
    "</font>\n",
    "</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbqznpN0QGVc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data pairs...\n",
      "Converting dimensions from 32x32 to 16x16\n",
      "Generating test data pairs...\n",
      "Converting dimensions from 32x32 to 16x16\n",
      "‚úÖ Data pair generation completed!\n",
      "üìä Training data dimensions - LR: (50000, 32, 32, 3), HR: (50000, 32, 32, 3)\n",
      "üìä Dimensions of test data - LR: (10000, 32, 32, 3), HR: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1VsmXnVn6OW"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">4. Data visualization</font> </div>\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "To better understand the data, display an example of low-quality images (the model input) and high-quality images (the desired output) side by side.\n",
    "</font>\n",
    "</div><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "knB-YqhvQGSr"
   },
   "outputs": [],
   "source": [
    "# TODO: Write a function to display an LR and HR image side by side.\n",
    "\n",
    "# TODO: Display a sample of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcgqxzA1QGNk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2wnPx74PIxu"
   },
   "source": [
    "<br /><br />\n",
    "\n",
    "<div dir=\"rtl\"><font size=\"5\">5. Building the Super-Resolution Model Architecture</font></div>\n",
    "<br />\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "In this section, we design the CNN model architecture for the <strong>Super-Resolution</strong> task.\n",
    "This model takes an <strong>LR</strong> image as input and produces a higher resolution (HR) image as output.\n",
    "<br /><br />\n",
    "<strong>Key architectural points:</strong>\n",
    "<li><strong>Input layer:</strong> Its dimensions should be proportional to the size of the LR images.</li>\n",
    "<li><strong>Feature extraction layers:</strong> Contains several <code>Conv2D</code> layers to learn patterns and textures in the image.</li>\n",
    "<li><strong>Upsampling layer:</strong> The <code>Conv2DTranspose</code> layer is used to increase the spatial dimensions of the feature map.\n",
    "This layer can, for example, convert a 16x16 map to 32x32.</li>\n",
    "<li><strong>Output layer:</strong> A <code>Conv2D</code> layer with 3 filters (for R, G, and B channels) and a <code>sigmoid</code> activation function\n",
    "that ensures that pixel values ‚Äã‚Äãremain in the range [0, 1].</li>\n",
    "<br /><br />\n",
    "The goal of designing a <strong>Super-Resolution</strong> model is to reconstruct high-resolution images from low-quality samples.\n",
    "In this process, the neural network learns to recover the details lost during quality reduction.\n",
    "<br /><br />\n",
    "This goal is usually achieved using architectures such as <strong>SRCNN</strong>, <strong>VDSR / EDSR</strong> and <strong>FSRCNN</strong>\n",
    "which use convolutional networks to extract features and methods such as <em>Residual Learning</em> to improve the final quality.\n",
    "<br /><br />\n",
    "However, since the concepts of <strong>Residual Connection</strong> and <strong>GAN</strong> have not been trained yet at this stage,\n",
    "we will use a simpler model to better understand the basic concept of Super-Resolution.\n",
    "In this model, the input image is first upscaled using an <code>UpSampling2D</code> or <code>Conv2DTranspose</code> layer\n",
    "and then several <code>Conv2D</code> layers are added to reconstruct subtle patterns and textures.\n",
    "Finally, the output layer with a <code>sigmoid</code> activation function keeps the pixel values ‚Äã‚Äãin the range [0, 1].\n",
    "<br /><br />\n",
    "This simple but effective architecture is a good introduction to understanding more advanced Super-Resolution models.\n",
    "<br><br>\n",
    "<font color=\"red\">Tip</font>: It is not necessary to use both layers at the same time. You can use either one of these layers. However, in more complex architectures and tasks, both layers may be used.\n",
    "</font>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wvmWas2PwsJ"
   },
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\">To read about the UpSampling2D and Conv2DTranspose layers, refer to the links below.</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAD5Me3rQIOa"
   },
   "source": [
    "* Basic Resouces:\n",
    "  * [Conv2DTranspose | Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose)\n",
    "  * [UpSampling2D | Tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D)\n",
    "\n",
    "* Advanced Resources:\n",
    "  * [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)\n",
    "  * [Understanding Transposed Convolution](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba/)\n",
    "\n",
    "  <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1l0hZpIzQk1d"
   },
   "outputs": [],
   "source": [
    "# TODO: Create Super Resolution function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bKXB7jpQTo3"
   },
   "outputs": [],
   "source": [
    "# TODO: Build the model\n",
    "\n",
    "# TODO: Show model architecture summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PN6G9D8HTxY6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKMGV00TQvI2"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">6. Model Training</font> </div>\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "Now we train the model using the prepared data. Use callbacks if needed.\n",
    "<br><br>\n",
    "<strong>Important Note:</strong>\n",
    "<li>The model input (x) is low-quality images.</li>\n",
    "<li>The target output (y) is high-quality images.</li>\n",
    "<li>The model learns to minimize the cost function between the predicted output and the actual output.</li>\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xLX3KYdRgMz"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement Callbacks functions if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4VVqUD7QTmR"
   },
   "outputs": [],
   "source": [
    "# TODO: Train the model.\n",
    "# Use the training data to train the model and the test data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayTZrWFOSVTr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D3_A2tVQ3dD"
   },
   "source": [
    "<br><br>\n",
    "<div dir=\"rtl\"> <font size=\"5\">7. Plotting the Cost Function (Loss)</font> </div>\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "Plot the cost function for the training and validation data to examine the learning process of the model.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxEtIhApQTjV"
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the Loss plot for the training and validation data.\n",
    "def plot_loss_history(history):\n",
    "    \"\"\"\n",
    "    Plots the Loss plot for the training and validation data.\n",
    "\n",
    "    Args:\n",
    "    history: The History object returned by the model.fit method.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2V4P1BzpQTgg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXQ9nV1tRQ3g"
   },
   "source": [
    "<br /><br />\n",
    "<div dir=\"rtl\"><font size=\"5\">8. Final Evaluation and Visualization of Results</font></div>\n",
    "<br />\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "In this section, you visually evaluate the performance of the Super-Resolution model.\n",
    "The goal is to see how well the model has reproduced details.\n",
    "<br /><br />\n",
    "<strong>Your task:</strong>\n",
    "<li>\n",
    "Use the trained model to make predictions on a few random samples of the test data.\n",
    "</li>\n",
    "<li>\n",
    "Then, for each sample, display three images side by side:\n",
    "<li>\n",
    "Low-Resolution (LR) input image: The image that the model received as input.\n",
    "</li>\n",
    "<li>\n",
    "Super-Res (Predicted) image: The image that your model predicted with high quality.\n",
    "</li>\n",
    "<li>\n",
    "Ground Truth HR: The original image that the model was supposed to reconstruct (the target of the model).\n",
    "</li>\n",
    "</li>\n",
    "By comparing these three images, you can evaluate the visual quality of the model's reconstruction and its ability to add detail.\n",
    "</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qEWYpA9QTd0"
   },
   "outputs": [],
   "source": [
    "# TODO: Predict high-quality images from low-quality test images using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrNUibukSyxi"
   },
   "outputs": [],
   "source": [
    "# TODO: Write a function to display a triplet of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt7rbcfpRVRb"
   },
   "source": [
    "<br /><br />\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"4\">\n",
    "<div dir=\"rtl\">\n",
    "<font size=\"5\">9. Quantitative Evaluation Using PSNR </font> </div>\n",
    "</font>\n",
    "<font size=\"4\">\n",
    "<strong>Peak Signal-to-Noise Ratio (PSNR)</strong> is an engineering metric for measuring the quality of image reconstruction that measures how close the reconstructed image (e.g., the output of a Super-Resolution model) is to the original image. This metric is defined based on the\n",
    "mean square error (MSE) and its unit is decibels (dB). The higher the\n",
    "PSNR value, the less noise and the better the reconstruction quality. <br /><br />\n",
    "Typically, a PSNR value higher than 30 dB indicates good quality in\n",
    "natural image reconstruction, while lower values ‚Äã‚Äãindicate the presence of noise or loss of detail in the image.\n",
    "<br /><br />\n",
    "Since the architecture designed in this exercise is a simple and educational model, there is no expectation of a high PSNR value. The main goal is to get acquainted with the basic concepts of the <strong>Super-Resolution</strong> task and how to evaluate it.\n",
    "<br><br>\n",
    "<font color='red'>Objective:</font> Try to achieve a PSNR score between 20 and 30.\n",
    "<br>\n",
    "<br>For more information on this score, see the following links.\n",
    "</font>\n",
    "</div>\n",
    "\n",
    "* [Peak Signal-to-Noise Ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n",
    "* [Tensorflow | PSNR](https://www.tensorflow.org/api_docs/python/tf/image/psnr)\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwJjP2yHRUfQ"
   },
   "outputs": [],
   "source": [
    "# TODO: Calculate PSNR for the entire test set\n",
    "# Use the psnr function from TensorFlow to calculate the Peak Signal-to-Noise Ratio (PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCGLCihxTJZ1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
